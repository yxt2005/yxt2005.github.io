<!DOCTYPE html>
<html lang=en>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>IMerge修改（Introduction） | Yang.x.t&#39;s Web</title>
  <meta name="description" content="Imerge修改（Introduction）1 introduction %背景介绍 1近年来，基于深度神经网络（DNN）的深度学习发展迅速。然而，随着数据规模和网络模型复杂度的指数级增长，在单一设备上完成训练已变得极其耗时。例如，在 ImageNet 数据集上使用单张 Nvidia Tesla V100 GPU 训练 ResNet-50 模型可能需要超过两天的时间 \cite&#123;wang">
<meta property="og:type" content="article">
<meta property="og:title" content="IMerge修改（Introduction）">
<meta property="og:url" content="http://example.com/2025/08/24/Imerge%E4%BF%AE%E6%94%B9/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Imerge修改（Introduction）1 introduction %背景介绍 1近年来，基于深度神经网络（DNN）的深度学习发展迅速。然而，随着数据规模和网络模型复杂度的指数级增长，在单一设备上完成训练已变得极其耗时。例如，在 ImageNet 数据集上使用单张 Nvidia Tesla V100 GPU 训练 ResNet-50 模型可能需要超过两天的时间 \cite&#123;wang">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-08-24T14:30:00.000Z">
<meta property="article:modified_time" content="2025-09-01T11:31:20.127Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  <!-- Canonical links -->
  <link rel="canonical" href="http://example.com/2025/08/24/Imerge%E4%BF%AE%E6%94%B9/index.html">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  
  
  
<meta name="generator" content="Hexo 7.3.0"></head>


<body class="main-center" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="https://github.com/yxt2005" target="_blank">
          <img class="img-circle img-rotate" src="/images/avatar.jpg" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">Yang.x.t.</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">XDUer</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> xi&#39;an, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="Search" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="Type something..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav ">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">Home</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">Archives</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">Categories</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags">
            
            <i class="icon icon-tags"></i>
            
            <span class="menu-title">Tags</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-repository">
          <a href="/repository">
            
            <i class="icon icon-project"></i>
            
            <span class="menu-title">Repository</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-books">
          <a href="/books">
            
            <i class="icon icon-book-fill"></i>
            
            <span class="menu-title">Books</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-links">
          <a href="/links">
            
            <i class="icon icon-friendship"></i>
            
            <span class="menu-title">Links</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-about">
          <a href="/about">
            
            <i class="icon icon-cup-fill"></i>
            
            <span class="menu-title">About</span>
          </a>
        </li>
        
      </ul>
      
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/cofess" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="http://weibo.com/cofess" target="_blank" title="Weibo" data-toggle=tooltip data-placement=top><i class="icon icon-weibo"></i></a></li>
        
        <li><a href="https://twitter.com/iwebued" target="_blank" title="Twitter" data-toggle=tooltip data-placement=top><i class="icon icon-twitter"></i></a></li>
        
        <li><a href="https://www.behance.net/cofess" target="_blank" title="Behance" data-toggle=tooltip data-placement=top><i class="icon icon-behance"></i></a></li>
        
        <li><a href="/atom.xml" target="_blank" title="Rss" data-toggle=tooltip data-placement=top><i class="icon icon-rss"></i></a></li>
        
    </ul>

    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">Board</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p>欢迎交流与分享经验!</p>
            </div>
        </div>
    </div>
</div>

    
      
  <div class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%EF%BC%88Hexo%EF%BC%89/">博客搭建（Hexo）</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">学习笔记 - 论文阅读</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7/">工具</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%AF%8F%E6%97%A5%E5%AD%A6%E4%B9%A0/">每日学习</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C-IMerge%E4%BF%AE%E6%94%B9/">论文写作 - IMerge修改</a><span class="category-list-count">5</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">Tags</h3>
    <div class="widget-body">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/" rel="tag">Hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Markdown/" rel="tag">Markdown</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/" rel="tag">博客搭建</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%96%87%E7%8C%AE%E7%AE%A1%E7%90%86/" rel="tag">文献管理</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" rel="tag">论文阅读</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/Hexo/" style="font-size: 13px;">Hexo</a> <a href="/tags/Markdown/" style="font-size: 13px;">Markdown</a> <a href="/tags/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/" style="font-size: 14px;">博客搭建</a> <a href="/tags/%E6%96%87%E7%8C%AE%E7%AE%A1%E7%90%86/" style="font-size: 13px;">文献管理</a> <a href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" style="font-size: 13px;">论文阅读</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">Archive</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/09/">September 2025</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/08/">August 2025</a><span class="archive-list-count">15</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget-body">
      <ul class="recent-post-list list-unstyled no-thumbnail">
        
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E5%B7%A5%E5%85%B7/">工具</a>
              </p>
              <p class="item-title">
                <a href="/2025/09/11/%E5%BF%AB%E9%80%9F%E6%88%AA%E5%B1%8F%E3%80%81%E6%A0%87%E6%B3%A8%E3%80%81%E5%A4%8D%E5%88%B6%E5%B7%A5%E5%85%B7/" class="title">Snipaste</a>
              </p>
              <p class="item-date">
                <time datetime="2025-09-11T12:30:00.000Z" itemprop="datePublished">2025-09-11</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E5%B7%A5%E5%85%B7/">工具</a>
              </p>
              <p class="item-title">
                <a href="/2025/09/11/%E7%99%BD%E9%B2%B8%E5%8A%A0%E9%80%9F%E5%99%A8%E7%9A%84%E5%AE%89%E8%A3%85/" class="title">白鲸加速器</a>
              </p>
              <p class="item-date">
                <time datetime="2025-09-11T11:30:00.000Z" itemprop="datePublished">2025-09-11</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E5%B7%A5%E5%85%B7/">工具</a>
              </p>
              <p class="item-title">
                <a href="/2025/09/11/Typora/" class="title">Typora 安装与使用</a>
              </p>
              <p class="item-date">
                <time datetime="2025-09-11T10:30:00.000Z" itemprop="datePublished">2025-09-11</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E5%B7%A5%E5%85%B7/">工具</a>
              </p>
              <p class="item-title">
                <a href="/2025/08/31/Visio%E7%9A%84%E4%B8%8B%E8%BD%BD%E5%AE%89%E8%A3%85%EF%BC%88%E5%9F%BA%E4%BA%8E%E8%A5%BF%E7%94%B5%E6%AD%A3%E7%89%88%E5%8C%96%E5%B9%B3%E5%8F%B0%EF%BC%89/" class="title">Visio的下载安装（基于西电正版化平台）</a>
              </p>
              <p class="item-date">
                <time datetime="2025-08-31T12:37:57.000Z" itemprop="datePublished">2025-08-31</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C-IMerge%E4%BF%AE%E6%94%B9/">论文写作 - IMerge修改</a>
              </p>
              <p class="item-title">
                <a href="/2025/08/30/IMerge-Evaluation%E9%83%A8%E5%88%86/" class="title">IMerge Evaluation部分</a>
              </p>
              <p class="item-date">
                <time datetime="2025-08-30T12:23:56.000Z" itemprop="datePublished">2025-08-30</time>
              </p>
            </div>
          </li>
          
      </ul>
    </div>
  </div>
  

    
  </div>
</aside>

  
  
<main class="main" role="main">
  <div class="content">
  <article id="post-Imerge修改" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      IMerge修改（Introduction）
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2025/08/24/Imerge%E4%BF%AE%E6%94%B9/" class="article-date">
	  <time datetime="2025-08-24T14:30:00.000Z" itemprop="datePublished">2025-08-24</time>
	</a>
</span>
        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C-IMerge%E4%BF%AE%E6%94%B9/">论文写作 - IMerge修改</a>
  </span>

        

        

        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2025/08/24/Imerge%E4%BF%AE%E6%94%B9/#comments" class="article-comment-link">Comments</a></span>
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <h1 id="Imerge修改（Introduction）"><a href="#Imerge修改（Introduction）" class="headerlink" title="Imerge修改（Introduction）"></a>Imerge修改（Introduction）</h1><p>1 introduction</p>
<p>%背景介绍</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">近年来，基于深度神经网络（DNN）的深度学习发展迅速。然而，随着数据规模和网络模型复杂度的指数级增长，在单一设备上完成训练已变得极其耗时。例如，在 ImageNet 数据集上使用单张 Nvidia Tesla V100 GPU 训练 ResNet-50 模型可能需要超过两天的时间 \cite&#123;wang2019benchmarking&#125;。因此，分布式深度学习的需求正稳步增长。</span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In recent years, deep learning based on deep neural networks (DNNs) has developed rapidly. However, with the exponential growth of data volume and network model complexity, training on a single device has become extremely time-consuming. For example, training the ResNet-50 model on the ImageNet dataset using a single Nvidia Tesla V100 GPU can take more than two days \cite&#123;wang2019benchmarking&#125;. Therefore, the demand for distributed deep learning has been steadily increasing.</span><br></pre></td></tr></table></figure>

<p>修改：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">近年来，基于深度神经网络（DNN）的深度学习快速发展，并被广泛应用于多个领域 [1]–[6]。然而，随着数据规模和模型复杂度的指数级增长，在单一设备上完成训练变得极为耗时 [7]。因此，为加速 DNN 的训练，分布式深度学习的需求日益增长 [8]。</span><br><span class="line">在分布式训练策略中，数据并行已成为最主流的方法 [9-12]。其基本思想是将训练数据划分给多个计算节点，每个节点独立执行前向与反向传播，并计算局部梯度。随后，这些梯度需要通过参数同步在所有节点之间进行聚合，以更新全局模型参数 [13], [14]。然而，在大规模分布式训练中，梯度聚合往往带来密集的通信开销，成为限制系统可扩展性的关键瓶颈 [13], [15]。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In recent years, deep learning based on deep neural networks (DNNs) has developed rapidly and has been widely applied in various domains~\cite&#123;krizhevsky2012imagenet,simonyan2014very,graves2013speech,amodei2016deep,deng2018deep,collobert2008unified&#125;. However, with the exponential growth of data volume and model complexity, training on a single device has become extremely time-consuming~\cite&#123;wang2019benchmarking&#125;. As a result, distributed deep learning has become increasingly important to accelerate the training of DNNs~\cite&#123;ben2019demystifying&#125;.</span><br><span class="line"></span><br><span class="line">Among distributed training strategies, data parallelism has emerged as the predominant approach~\cite&#123;shallue2019measuring,dean2012large,dekel2012optimal,meng2019convergence&#125;. The basic idea is to partition the training data across multiple workers, where each worker performs forward and backward propagation independently and computes local gradients. These gradients must then be aggregated through parameter synchronization across all workers to update the global model parameters~\cite&#123;shi2021exploiting,shi2018performance&#125;. However, in large-scale distributed training, gradient aggregation often introduces intensive communication overhead, which becomes a key bottleneck that limits scalability~\cite&#123;shi2021exploiting,duan2022mercury&#125;.</span><br><span class="line"></span><br><span class="line">Therefore, parameter synchronization has become a critical factor affecting the performance of distributed deep learning.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>%PS vs. All-Reduce,(引入两种架构，介绍PS优势)</p>
<p>在分布式深度学习中，有两种最流行的系统架构：All-Reduce架构和参数服务器架构，分别采用去中心化和中心化的设计思想，具体定义了计算节点间如何通信、与谁通信等一系列完整的参数同步规则。</p>
<p>在All-Reduce 架构中，参数同步的过程直接在 worker 节点内部执行，通过去中心化的点对点通信实现全局梯度聚合。Allreduce 有多种实现方式，其中最为著名的是 ring allreduce，它通过将张量分成小消息并以流水线方式同时交换这些消息来实现带宽优化。然而，它主要适用于传输少量的大消息，事实上对于一个分布式训练任务而言，传输大量的小消息是更为普遍的现象。通信次数和启动时延随集群规模线性增长。此外 ring 算法的链式通信结构也使其容灾能力较差，限制其扩展性。</p>
<p>相比之下，PS 架构采用集中式参数管理方式，在容错性、灵活性以及异构环境适配方面展现出独特优势。PS在逻辑上是一个中央服务器，它聚合来自worker的梯度，更新模型参数，并将最新的模型发回给worker，为系统实现提供了一个简单而灵活的框架。</p>
<hr>
<p>修改版本（适当精简内容，与前面背景描述衔接）：</p>
<p>%PS与All-Reduce的对比</p>
<p>因此，在分布式深度学习中，参数同步是影响训练性能的核心环节。目前最常见的两类系统架构是 <strong>参数服务器（Parameter Server, PS）</strong> 和 <strong>All-Reduce</strong>。All-Reduce 架构通过去中心化的点对点通信实现全局梯度聚合，能够高效利用带宽。然而，它更适合少量大消息的传输，而在实际分布式训练中，大量小消息的通信更为常见，因而其优势受限。同时，其通信次数与启动延迟随集群规模线性增长，其链式结构在容错性上也较为薄弱，从而进一步限制了系统的可扩展性。相比之下，PS 架构采用集中式参数管理方式，在容错性、灵活性以及异构环境适配方面展现出独特优势。</p>
<figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In distributed deep learning, parameter synchronization is a critical factor that affects training performance. The two most common system architectures are the <span class="keyword">\textbf</span>&#123;Parameter Server (PS)&#125; and <span class="keyword">\textbf</span>&#123;All-Reduce&#125;. The All-Reduce architecture~<span class="keyword">\cite</span>&#123;awan2017s,hoefler2010toward,rabenseifner2004optimization,jia2018highly&#125; performs global gradient aggregation through decentralized peer-to-peer communication, which enables efficient bandwidth utilization. However, it is more suitable for transmitting a small number of large messages, whereas in practical distributed training, the communication of numerous small messages is more common, which limits its advantages. Meanwhile, its communication rounds and startup latency grow linearly with cluster size, and its chain-based structure exhibits weak fault tolerance, further constraining system scalability. In contrast, the PS architecture<span class="keyword">\cite</span>&#123;dean2012large, smola2010architecture,li2013parameter,li2014communication &#125; adopts a centralized parameter management approach, demonstrating unique advantages in fault tolerance, flexibility, and adaptability to heterogeneous environments.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<hr>
<p>%PS的问题</p>
<p>然而，原生的 PS 架构在设计上也存在不足。首先，其高度中心化的多对一通信模式容易造成带宽瓶颈与网络拥塞，使单个 PS 节点成为分布式系统扩展效率的关键瓶颈，从而显著限制整体的可扩展性与训练性能。其次，计算与通信之间的强串行关系也使得集群的资源利用率普遍不高。为了能更高效地发挥出多算力设备并行处理的优势，减少非必要的通信开销，研究参数服务器的通信优化问题具有重要的应用价值。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">However, the original PS architecture also suffers from inherent limitations. Its highly centralized many-to-one communication pattern can lead to bandwidth bottlenecks and network congestion, making a single PS node the critical bottleneck that restricts system scalability and significantly degrades training performance.</span><br></pre></td></tr></table></figure>

<p>修改版本（增加计算与通信的强串行关系问题）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">However, the original PS architecture exhibits inherent drawbacks.First, its highly centralized many-to-one communication pattern tends to incur bandwidth bottlenecks and network congestion, rendering a single PS node the critical factor that limits system scalability and impedes training performance. Second, the strong serialization between computation and communication often leads to suboptimal resource utilization across the cluster. Therefore, optimizing communication in PS-based systems is of practical significance for fully exploiting parallelism across multiple computing devices while reducing unnecessary communication overhead.</span><br></pre></td></tr></table></figure>

<p>%相关工作：</p>
<p>最近的研究主要从 通信架构优化、通信压缩 和 通信调度 三个方面对参数服务器进行改进，以提升其通信效率和系统扩展性。</p>
<p>在 <strong>通信架构优化</strong> 方面，核心思想是改进参数服务器运行时遵循的逻辑拓扑。在默认的星型拓扑下，单一 PS 节点作为中心汇聚所有 worker 的梯度与参数更新，极易因带宽压力成为性能瓶颈。已有工作主要沿两条路径展开：一类方法通过增加 PS 节点数量实现横向扩展，如 BytePS；另一类方法通过引入多级服务器实现纵向扩展，以缓解跨层通信负载。</p>
<p>在 <strong>通信压缩</strong> 方面，研究者尝试通过减少传输数据量来降低通信开销。常见做法包括梯度量化，将高比特梯度转换为低比特表示，以及梯度稀疏化，只传输绝对值超过阈值的梯度。尽管这些方法能够显著减少通信量，但也可能导致模型精度下降，需要梯度补偿机制来弥补，同时压缩与解压缩过程本身会引入额外计算开销，从而在某些场景下削弱整体加速效果。</p>
<p>在 <strong>通信调度</strong> 方面，主要思想是打破计算与通信的串行关系，通过任务调度实现二者的重叠。DNN 训练通常可以抽象为有向无环图，其中计算（前向和反向传播）与通信（梯度聚合）占用的资源不同，因此通信可隐藏在计算过程中。在不影响模型收敛的前提下，已有方法分别探索了通信与反向传播阶段的重叠，以及通信与前向传播阶段的重叠，从而缩短一次迭代所需时间。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">%相关工作：通信架构优化、通信压缩、通信调度</span><br><span class="line">Recent research has primarily focused on three directions to optimize the Parameter Server (PS) architecture, </span><br><span class="line">namely \textbf&#123;communication architecture optimization&#125;, \textbf&#123;communication compression&#125;, and \textbf&#123;communication scheduling&#125;, </span><br><span class="line">with the aim of improving communication efficiency and system scalability.  </span><br><span class="line"></span><br><span class="line">%通信架构优化</span><br><span class="line">For \textbf&#123;communication architecture optimization&#125;, the core idea is to refine the logical topology followed by PS during runtime. </span><br><span class="line">In the default star topology\cite&#123;kairouz2021advances&#125;, a single PS node serves as the central aggregator for gradients and parameter updates from all workers, which easily becomes a performance bottleneck due to bandwidth pressure. </span><br><span class="line">Existing works mainly follow two approaches: one line of research increases the number of PS nodes to achieve horizontal scaling, as exemplified by BytePS\cite&#123;jiang2020unified&#125;; </span><br><span class="line">another introduces hierarchical PS designs\cite&#123;wan2020rat,luo2020plink,yang2018tree,huang2021communication&#125; to enable vertical scaling and alleviate cross-layer communication overhead.  </span><br><span class="line"></span><br><span class="line">%通信压缩</span><br><span class="line">In terms of \textbf&#123;communication compression&#125;, researchers aim to reduce the volume of transmitted data to lower communication costs. </span><br><span class="line">Typical techniques include gradient quantization\cite&#123;seide20141, wen2017terngrad&#125; , which converts high-precision gradients into low-bit representations, </span><br><span class="line">and gradient sparsification\cite&#123;strom2015scalable, aji2017sparse, shi2020layer&#125;, which transmits only gradients whose absolute values exceed a predefined threshold. </span><br><span class="line">Although these methods can significantly reduce communication overhead, they may lead to accuracy degradation\cite&#123;grubic2018synchronous&#125;, </span><br><span class="line">which often requires error compensation mechanisms to mitigate. </span><br><span class="line">Furthermore, the compression and decompression processes introduce additional computational overhead, </span><br><span class="line">which in some cases offsets the benefits of communication reduction.  </span><br><span class="line"></span><br><span class="line">%通信调度</span><br><span class="line">Regarding \textbf&#123;communication scheduling&#125;, the main idea is to break the strict serialization between computation and communication by overlapping the two. </span><br><span class="line">The training of deep neural networks (DNNs) can be abstracted as a directed acyclic graph (DAG), </span><br><span class="line">where computation (forward and backward propagation) and communication (gradient aggregation) utilize different resources. </span><br><span class="line">As a result, communication can be overlapped with computation without affecting convergence. </span><br><span class="line">Existing studies have explored overlapping communication with the backward propagation as well as with the forward propagation, </span><br><span class="line">thereby reducing the iteration time.  </span><br></pre></td></tr></table></figure>

<p>首段（引出相关工作部分）修改：不要有过多加粗，改为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Recent research has primarily focused on three directions to optimize the Parameter Server (PS) architecture, namely (i) communication architecture optimization, (ii) communication compression, and (iii) communication scheduling, with the aim of improving communication efficiency and system scalability.</span><br></pre></td></tr></table></figure>

<p>通信调度修改：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在通信调度方面，主要思想是打破计算与通信的串行关系，通过任务调度实现二者的重叠。在分布式深度学习的训练过程中，计算任务和通信任务可以用有向无环图</span><br><span class="line">(DAG)来描述[]。其中计算任务（前向和反向传播）与通信任务（梯度聚合）占用的资源不同，因此通信可隐藏在计算过程中，从而隐藏部分通信开销[]。在不影响模型收敛的前提下，已有方法分别探索了通信与反向传播阶段的重叠，以及通信与前向传播阶段的重叠，从而缩短一次迭代所需时间。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Regarding \textbf&#123;communication scheduling&#125;, the main idea is to break the strict serialization between computation and communication by overlapping the two through task scheduling. </span><br><span class="line">In distributed deep learning training, computation tasks and communication tasks can be represented by a directed acyclic graph (DAG)\cite&#123;shi2018dag&#125;. </span><br><span class="line">Since computation (forward and backward propagation) and communication (gradient aggregation) utilize different resources, communication can be overlapped with computation to hide part of the communication overhead\cite&#123;hashemi2019tictac,jayarajan2019priority,li2018pipe,narayanan2019pipedream,sergeev2018horovod,peng2019generic,shi2020communication&#125;. </span><br><span class="line">Without affecting model convergence, existing methods have explored overlapping communication with the backward propagation as well as with the forward propagation, thereby reducing the iteration time.</span><br></pre></td></tr></table></figure>

<p>%过渡</p>
<p>尽管在通信架构优化、通信压缩和通信调度方面已有许多研究工作，并在一定程度上缓解了参数服务器的通信瓶颈，但在中心化 PS 架构下仍显不足。特别是多对一的通信模式依然导致带宽瓶颈和尾延迟，严重制约了系统的可扩展性和效率。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

<p>修改版本</p>
<p>%我们的工作</p>
<p>为此，本文提出 IMerge —— 一种结合梯度融合与交错通信的无等待反向传播算法，有效缓解通信延迟与带宽瓶颈。</p>
<p>首先，我们采用梯度融合策略，将部分相邻层的参数合并发送以减少启动延迟，并通过建立准确合理的融合条件与通信模型，作为动态判别融合层的依据。其次，针对参数服务器架构下潜在的网络拥塞问题，我们设计一种交错通信的调度策略，包括迭代之间的交错和迭代之内的交错，以减少每轮迭代中通信的参数量和并发数，从而缓解不同工作节点对网络资源的竞争。最后，我们基于Pytorch设计了一种包含多 PS 节点的参数服务器原型系统以分担通信带宽的压力，并采用一种参数轮询分配的机制保证各 PS 节点的工作负载均衡。通过将交错融合通信策略集成至本文搭建的参数服务器中，可有效提升系统的训练效率，实验结果表明本文设计的方案相较默认的参数服务器同步训练机制对于 ResNet18、Vgg16 和 AlexNet 模型分别可获得约 84.4%、211.5%和 100.9%的性能加速效果。</p>
<p>最新版本：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">\section&#123;Introduction&#125;</span><br><span class="line">\label&#123;sec:introduction&#125;</span><br><span class="line"></span><br><span class="line">% 背景描述</span><br><span class="line">In recent years, deep learning based on deep neural networks (DNNs) has developed rapidly and has been widely applied in various domains~\cite&#123;krizhevsky2012imagenet,simonyan2014very,graves2013speech,amodei2016deep,deng2018deep,collobert2008unified&#125;. However, with the exponential growth of data volume and model complexity, training on a single device becomes prohibitively time-consuming~\cite&#123;wang2019benchmarking&#125;. As a result, distributed deep learning has become increasingly important to accelerate the training of DNNs~\cite&#123;ben2019demystifying&#125;.</span><br><span class="line"></span><br><span class="line">Among distributed training strategies, data parallelism has emerged as the predominant approach~\cite&#123;shallue2019measuring,dean2012large,dekel2012optimal,meng2019convergence&#125;. The basic idea is to partition the training data across multiple workers, where each worker performs forward and backward propagation independently and computes local gradients. These gradients must then be aggregated through parameter synchronization across all workers to update the global model parameters~\cite&#123;shi2021exploiting,shi2018performance&#125;. However, in large-scale distributed training, gradient aggregation often introduces intensive communication overhead, which emerges as a key scalability bottleneck~\cite&#123;shi2021exploiting, duan2022mercury&#125;.</span><br><span class="line"></span><br><span class="line">% PS vs. all-reduce</span><br><span class="line">Therefore, parameter synchronization has become a critical factor affecting the performance of distributed deep learning. The two most common system architectures are the \textbf&#123;Parameter Server (PS)&#125; and \textbf&#123;All-Reduce&#125;. The All-Reduce architecture~\cite&#123;awan2017s,hoefler2010toward,rabenseifner2004optimization,jia2018highly&#125; performs global gradient aggregation through decentralized peer-to-peer communication, which enables efficient bandwidth utilization. However, it is more suitable for transmitting a small number of large messages, whereas in practical distributed training, the communication of numerous small messages is more common, limiting its advantages. Meanwhile, its communication rounds and startup latency grow linearly with cluster size, and its chain-based structure exhibits weak fault tolerance, further constraining scalability. In contrast, the PS architecture~\cite&#123;smola2010architecture,li2013parameter,li2014communication&#125; adopts a centralized parameter management approach, demonstrating unique advantages in fault tolerance, flexibility, and adaptability to heterogeneous environments.</span><br><span class="line"></span><br><span class="line">% 参数服务器架构问题</span><br><span class="line">However, the original PS architecture exhibits inherent limitations. First, its highly centralized many-to-one communication pattern often leads to bandwidth bottlenecks and network congestion, making a single PS node the critical bottleneck, which constrains scalability and degrades training performance. Second, the strong serialization between computation and communication results in suboptimal resource utilization across the cluster. Therefore, optimizing communication in PS-based systems is of practical importance for fully leveraging the parallelism of multiple computing devices while reducing unnecessary overhead.</span><br><span class="line"></span><br><span class="line">%相关工作：通信架构优化、通信压缩、通信调度</span><br><span class="line">Recent research has primarily focused on three directions to optimize the PS architecture, namely (i) communication architecture optimization, (ii) communication compression, and (iii) communication scheduling, with the aim of improving efficiency and scalability.</span><br><span class="line"></span><br><span class="line">%通信架构优化</span><br><span class="line">For \textbf&#123;communication architecture optimization&#125;, the core idea is to refine the logical topology followed by PS during runtime. </span><br><span class="line">In the default star topology~\cite&#123;kairouz2021advances&#125;, a single PS node serves as the central aggregator for gradients and parameter updates from all workers, which easily becomes a performance bottleneck due to bandwidth pressure. </span><br><span class="line">Existing works mainly follow two approaches: one line of research increases the number of PS nodes to achieve horizontal scaling, as exemplified by BytePS~\cite&#123;jiang2020unified&#125;; </span><br><span class="line">another introduces hierarchical PS designs~\cite&#123;wan2020rat,luo2020plink,yang2018tree,huang2021communication&#125; to enable vertical scaling and alleviate cross-layer communication overhead.  </span><br><span class="line"></span><br><span class="line">%通信压缩</span><br><span class="line">In terms of \textbf&#123;communication compression&#125;, researchers aim to reduce the volume of transmitted data to lower communication costs. </span><br><span class="line">Typical techniques include gradient quantization~\cite&#123;seide20141, wen2017terngrad&#125;, which converts high-precision gradients into low-bit representations, </span><br><span class="line">and gradient sparsification~\cite&#123;strom2015scalable, aji2017sparse, shi2020layer&#125;, which transmits only gradients whose absolute values exceed a predefined threshold. </span><br><span class="line">Although these methods can significantly reduce communication overhead, they may cause accuracy degradation~\cite&#123;grubic2018synchronous&#125;, </span><br><span class="line">which often requires error compensation mechanisms. </span><br><span class="line">Furthermore, the compression and decompression processes introduce additional computation, which in some cases offsets the benefits of communication reduction.  </span><br><span class="line"></span><br><span class="line">%通信调度</span><br><span class="line">Regarding \textbf&#123;communication scheduling&#125;, the main idea is to break the strict serialization between computation and communication by overlapping the two through task scheduling. </span><br><span class="line">In distributed deep learning training, computation tasks and communication tasks can be represented by a directed acyclic graph (DAG)~\cite&#123;shi2018dag&#125;. </span><br><span class="line">Since computation (forward and backward propagation) and communication (gradient aggregation) utilize different resources, communication can be overlapped with computation to hide part of the overhead~\cite&#123;hashemi2019tictac,jayarajan2019priority,li2018pipe,narayanan2019pipedream,sergeev2018horovod,peng2019generic,shi2020communication&#125;. </span><br><span class="line">Without affecting model convergence, existing methods have explored overlapping communication with the backward propagation as well as with the forward propagation, thereby reducing iteration time.</span><br><span class="line"></span><br><span class="line">%由当前工作问题过渡到我们的工作</span><br><span class="line">Although prior efforts on communication architecture optimization, compression, and scheduling have alleviated the bottlenecks of PS to some extent, they remain insufficient under the centralized PS architecture. In particular, the many-to-one communication pattern still causes bandwidth saturation and long-tail latency, posing critical challenges to scalability and efficiency. </span><br><span class="line"></span><br><span class="line">% 我们的工作</span><br><span class="line">To address these issues, in this paper, we propose \textbf&#123;IMerge&#125;, an Interleaved-communication and Merged-gradient Wait-Free Backpropagation (WFBP) algorithm that integrates gradient fusion with interleaved scheduling to mitigate both latency and bandwidth constraints in PS-based distributed deep learning. </span><br><span class="line">First, we introduce a \textbf&#123;gradient fusion strategy&#125; that aggregates parameters from adjacent layers to reduce startup latency. We establish a mathematical communication model and well-defined fusion conditions, which dynamically determine whether layers should be fused during training.  </span><br><span class="line">Second, to mitigate potential network congestion in PS-based systems, we design an \textbf&#123;interleaved communication scheduling strategy&#125; that operates both across iterations and within a single iteration. This strategy reduces the volume and concurrency of parameter transmissions per round, thereby alleviating contention among workers for network resources.  </span><br><span class="line">Third, we implement a \textbf&#123;PyTorch-based multi-PS prototype&#125; to distribute communication bandwidth. A parameter polling mechanism is adopted to balance workloads across servers and prevent hotspot bottlenecks. By integrating our interleaved and fused communication strategy into this prototype, we effectively decouple computation and communication.  </span><br><span class="line">Extensive experiments demonstrate that our approach significantly improves training efficiency compared to the default PS-based synchronization mechanism. On ResNet18, VGG16, and AlexNet, our method achieves performance speedups of approximately 84.4\%, 211.5\%, and 100.9\%, respectively.  </span><br><span class="line"></span><br><span class="line">%我们的贡献总结</span><br><span class="line">Our contributions are summarized as follows:</span><br><span class="line">\begin&#123;itemize&#125;</span><br><span class="line">	\item We provide an in-depth analysis of communication scheduling strategies in distributed deep learning, investigating how parameter transmission granularity and timing affect communication performance and system scalability, and comparing the characteristics of different approaches.</span><br><span class="line">	</span><br><span class="line">	\item We propose \textbf&#123;IMerge&#125;, a novel scheduling strategy that integrates gradient fusion with interleaved communication. IMerge leverages a PS-specific model to predict the computation--communication timeline and dynamically determine fusion conditions, while its interleaved scheduling mechanism reduces bandwidth contention and overlaps communication with computation to sustain high throughput.</span><br><span class="line">	</span><br><span class="line">	\item We develop a PyTorch-based multi-PS prototype system with a parameter polling mechanism for balanced load distribution. By integrating IMerge into this prototype, we achieve significant performance improvements on CNN benchmarks, including ResNet18, VGG16, AlexNet, and ResNet50.</span><br><span class="line">\end&#123;itemize&#125;</span><br><span class="line"></span><br><span class="line">% 论文结构</span><br><span class="line">The remainder of this paper is organized as follows. Section \ref&#123;sec:Background and Motivation&#125; introduces the background and motivation, including distributed deep learning and the PS architecture. Section \ref&#123;sec:IMerge Design&#125; describes the design of the IMerge algorithm, including gradient fusion and interleaved communication strategies. Section \ref&#123;sec:Implementation&#125; presents the system implementation based on a multi-PS prototype. Section \ref&#123;sec:Evaluation&#125; reports experimental results and performance evaluations. Finally, Section \ref&#123;sec:Conclusions&#125; concludes the paper and discusses future directions.</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="2025-09-01"><a href="#2025-09-01" class="headerlink" title="2025&#x2F;09&#x2F;01"></a>2025&#x2F;09&#x2F;01</h2><p>目标：修改贡献总结部分</p>
<p>（1）与我们的实际实验（成果）一一对应</p>
<p>参考老师点评：我们针对计算通信重叠的问题：融合通信（新的融合模型对通信的评估更加准确、精确控制）、多并发导致的网络拥塞提出交错通信。最后，我们基于Pytorch搭建了多参数服务器系统，并整合我们的交错融合通信策略，</p>
<p><strong>先修改我们的工作介绍部分：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">为此，本文提出 IMerge —— 一种结合梯度融合与交错通信的无等待反向传播算法，在参数服务器架构下有效缓解通信延迟与带宽瓶颈问题。</span><br><span class="line"></span><br><span class="line">首先，我们采用融合梯度通信策略，将部分相邻层的小参数合并发送以减少启动延迟。</span><br><span class="line">针对传统的线性通信模型 [xx] 难以准确刻画参数服务器架构下“多对一”通信模式的不足，本文建立了一个新的通信模型，能够更好地适配 PS 架构，并准确预测模型训练过程中计算与通信的时间线（timeline），为动态判断融合条件提供理论依据。</span><br><span class="line">其次，为进一步缓解参数服务器架构下潜在的网络拥塞和带宽瓶颈问题，我们在融合通信的基础上设计了一种交错通信策略，涵盖迭代之间与迭代之内的两种交错调度机制，以减少每轮迭代中通信的参数量和并发数，从而缓解不同工作节点对网络资源的竞争。</span><br><span class="line">最后，我们基于Pytorch搭建了包含多 PS 节点的参数服务器实物平台，并采用一种参数轮询分配的机制保证各 PS 节点的工作负载均衡，同时将所提出的交错与融合通信策略（IMerge）集成至该系统中。</span><br><span class="line">为了验证我们提出的IMerge的有效性，我们在一个一个最大规模为32个节点的GPU集群上用4个典型的CNNs来评估其性能。</span><br><span class="line">实验结果表明，在 32 节点规模的参数服务器训练平台下，基于本文建立的新的通信模型的融合梯度通信策略相较于基于传统的通信模型的融合通信策略可获得 28.5\%-182.3%的加速效果。</span><br><span class="line">进一步，本文提出的IMerge相较默认的参数服务器同步训练机制可获得约 84.4%-211.5%的加速效果。</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>对应英文：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">To address these issues, in this paper, we propose \textbf&#123;IMerge&#125;—an interleaved-communication and merged-gradient wait-free backpropagation (WFBP) algorithm designed to alleviate communication latency and bandwidth bottlenecks in parameter server (PS) architectures.</span><br><span class="line"></span><br><span class="line">First, we introduce a \textbf&#123;gradient fusion strategy&#125; that merges small parameters from adjacent layers to reduce startup latency. Observing that traditional linear communication models [xx] fail to accurately characterize the many-to-one communication pattern inherent to PS-based systems, we establish a new communication model tailored for the PS architecture. This model enables accurate prediction of the computation–communication timeline during training and provides a theoretical basis for dynamically determining fusion conditions.</span><br><span class="line"></span><br><span class="line">Second, to further mitigate potential network congestion and bandwidth contention in PS systems, we propose an  \textbf&#123;interleaved communication scheduling strategy&#125; on top of gradient fusion. This strategy includes both inter-iteration and intra-iteration interleaving mechanisms, which reduce the communication volume and concurrency per iteration, thereby alleviating contention among workers for network resources.</span><br><span class="line"></span><br><span class="line">Finally, we implement a multi-PS prototype system based on PyTorch, incorporating a parameter polling mechanism to balance workloads across PS nodes. The proposed interleaved and fused communication strategies (IMerge) are fully integrated into this system.</span><br><span class="line"></span><br><span class="line">To evaluate the effectiveness of IMerge, we conduct experiments on a GPU cluster with up to 32 nodes using four representative CNN models. Experimental results show that, under the 32-node PS training setting, our fusion strategy guided by the proposed new communication model achieves 28.5%–182.3% speedup over the same strategy guided by the traditional communication model. Furthermore, IMerge achieves 84.4%–211.5% performance improvement over the default PS-based synchronization mechanism.</span><br></pre></td></tr></table></figure>

<p><strong><u>遗留问题：目前整体稍显较长，可以适当压缩</u></strong></p>
<p>2.<strong>修改我们的贡献总结部分</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">本文的主要贡献如下：</span><br><span class="line"></span><br><span class="line">本文提出了一种新的通信调度算法 IMerge，适用于参数服务器架构的分布式训练。</span><br><span class="line"></span><br><span class="line">我们建立了一种新的通信模型，更准确地刻画了参数服务器的多对一通信模式，并在此基础上设计了更加准确的梯度融合策略。</span><br><span class="line"></span><br><span class="line">在融合通信策略的基础上，进一步设计了交错通信调度，缓解参数服务器架构下潜在的网络拥塞和带宽瓶颈问题。</span><br><span class="line"></span><br><span class="line">我们基于 PyTorch 搭建了多 PS 原型系统并集成 IMerge，在一个 32 节点的 GPU 集群上进行真实实验评估其性能。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Our main contributions are summarized as follows:</span><br><span class="line">\begin&#123;itemize&#125;</span><br><span class="line">    \item We propose a new communication scheduling algorithm, \textbf&#123;IMerge&#125;, for distributed training under the parameter server architecture.</span><br><span class="line"></span><br><span class="line">    \item We establish a new communication model that more accurately characterizes the many-to-one transmission pattern in PS systems, based on which we design a model-guided gradient fusion strategy.</span><br><span class="line"></span><br><span class="line">    \item On top of the fusion strategy, we further design an interleaved communication scheduling strategy that incorporates both inter-iteration and intra-iteration interleaving mechanisms.</span><br><span class="line"></span><br><span class="line">    \item We implement a PyTorch-based multi-PS prototype system, integrate IMerge into it, and conduct real-world experiments on a 32-node GPU cluster to evaluate its performance.</span><br><span class="line">\end&#123;itemize&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>


      
    </div>
    <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="http://example.com/2025/08/24/Imerge%E4%BF%AE%E6%94%B9/" title="IMerge修改（Introduction）" target="_blank" rel="external">http://example.com/2025/08/24/Imerge修改/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="https://github.com/yxt2005" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/avatar.jpg" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="https://github.com/yxt2005" target="_blank"><span class="text-dark">Yang.x.t.</span><small class="ml-1x">XDUer</small></a></h3>
        <div>2023级西安电子科技大学本科生，通信工程专业。</div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    
  <section id="comments">
  	
      <div id="vcomments"></div>
    
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    <li class="prev">
      <a href="/2025/08/26/IMerge%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E6%95%B0%E6%8D%AE%E4%B8%8E%E7%BB%98%E5%9B%BE/" title="IMerge实验结果数据与绘图"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;Newer</span></a>
    </li>
    
    
    <li class="next">
      <a href="/2025/08/24/Hexo%20%E6%9C%AC%E5%9C%B0%E5%8D%9A%E5%AE%A2%E4%B8%8A%E4%BC%A0%E4%B8%8E%E6%96%B0%E5%BB%BA%E6%96%87%E7%AB%A0%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/" title="Hexo 本地博客上传与新建文章简明教程"><span>Older&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
  </ul>
  
  
  <!-- Button trigger modal -->
  <button type="button" class="btn btn-fancy btn-donate pop-onhover bg-gradient-warning" data-toggle="modal" data-target="#donateModal"><span>$</span></button>
  <!-- <div class="wave-icon wave-icon-danger btn-donate" data-toggle="modal" data-target="#donateModal">
    <div class="wave-circle"><span class="icon"><i class="icon icon-bill"></i></span></div>
  </div> -->
  
  
  <div class="bar-right">
    
    <div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div>
    
  </div>
  </div>
</nav>
  
<!-- Modal -->
<div class="modal modal-center modal-small modal-xs-full fade" id="donateModal" tabindex="-1" role="dialog">
  <div class="modal-dialog" role="document">
    <div class="modal-content donate">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
      <div class="modal-body">
        <div class="donate-box">
          <div class="donate-head">
            <p>Maybe you could buy me a cup of coffee.</p>
          </div>
          <div class="tab-content">
            <div role="tabpanel" class="tab-pane fade active in" id="alipay">
              <div class="donate-payimg">
                <img src="/images/donate/allipay.jpg" alt="Scan Qrcode" title="Scan" />
              </div>
              <p class="text-muted mv">Scan this qrcode</p>
              <p class="text-grey">Open alipay app scan this qrcode, buy me a coffee!</p>
            </div>
            <div role="tabpanel" class="tab-pane fade" id="wechatpay">
              <div class="donate-payimg">
                <img src="/images/donate/wechatpay.jpg" alt="Scan Qrcode" title="Scan" />
              </div>
              <p class="text-muted mv">Scan this qrcode</p>
              <p class="text-grey">Open wechat app scan this qrcode, buy me a coffee!</p>
            </div>
          </div>
          <div class="donate-footer">
            <ul class="nav nav-tabs nav-justified" role="tablist">
              <li role="presentation" class="active">
                <a href="#alipay" id="alipay-tab" role="tab" data-toggle="tab" aria-controls="alipay" aria-expanded="true"><i class="icon icon-alipay"></i> alipay</a>
              </li>
              <li role="presentation" class="">
                <a href="#wechatpay" role="tab" id="wechatpay-tab" data-toggle="tab" aria-controls="wechatpay" aria-expanded="false"><i class="icon icon-wepay"></i> wechat payment</a>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>



</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/cofess" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="http://weibo.com/cofess" target="_blank" title="Weibo" data-toggle=tooltip data-placement=top><i class="icon icon-weibo"></i></a></li>
        
        <li><a href="https://twitter.com/iwebued" target="_blank" title="Twitter" data-toggle=tooltip data-placement=top><i class="icon icon-twitter"></i></a></li>
        
        <li><a href="https://www.behance.net/cofess" target="_blank" title="Behance" data-toggle=tooltip data-placement=top><i class="icon icon-behance"></i></a></li>
        
        <li><a href="/atom.xml" target="_blank" title="Rss" data-toggle=tooltip data-placement=top><i class="icon icon-rss"></i></a></li>
        
    </ul>

    <div class="copyright">
    	
        <div class="publishby">
        	Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>


<script src="/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>






   




   
    
  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/valine"></script>
  <script type="text/javascript">
  var GUEST = ['nick', 'mail', 'link'];
  var meta = 'nick,mail,link';
  meta = meta.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#vcomments',
    verify: false,
    notify: false,
    appId: '',
    appKey: '',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: meta,
    pageSize: '10' || 10,
    visitor: false
  });
  </script>

     







</body>
</html>